{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disco  \n",
    "Copyright (C) 2022-present NAVER Corp.  \n",
    "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expressing Preferences via an EBM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to express our preferences on the generated sequences. We do this via an EBM obtained by constraining a base model with scoring features.\n",
    "We're going to reuse the amazing example from the [README](README.md) to go in a little more depth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise Constraint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first revisit the amazing example from the [README](README.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a `BooleanScorer`, it's straightforward to express that we want to have \"amazing\" in the sampled texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.scorers import BooleanScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_scorer = BooleanScorer(lambda s, c: \"amazing\" in s.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already (log-)score text samples, using a named tuple defined in disco to format them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.distributions.lm_distribution import TextSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "        TextSample(list(), \"This is quite amazing.\"),\n",
    "        TextSample(list(), \"This is amazingly relevant.\"),\n",
    "        TextSample(list(), \"This is the toolkit at work.\")\n",
    "    ]\n",
    "amazing_scorer.log_score(samples, '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The lists as first members of the tuples are empty as we don't need the tokenized form to score the samples with amazing_scorer.\n",
    "1. We pass an empty context when (log-)scoring as it's not relevant when looking for amazing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we've again simplied things a bit here as we're not scoring the presence of \"amazing\" as word but of the string. Can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_amazing = lambda s, c: bool(re.search(r\"\\bamazing\\b\", s.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_scorer = BooleanScorer(is_amazing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_scorer.log_score(samples, '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this scorer, we can express our preference in an EBM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by instantiating a LMDistribution as the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.distributions import LMDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = LMDistribution()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify that all our generated samples should include the word \"amazing\", we then use a straight product to define our target EBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base * amazing_scorer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we've only specified our preferences, or constraints: we would then have to approximate this EBM, for example by tuning a model â€”although we've expressed that we want all our samples to include \"amazing\", we will only approach that constraint, up to about 80% from our experiments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Constraint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only want half of our samples to include amazing, we have to constrain the base model to compute the coefficient to use in the resulting EBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([amazing_scorer], [1/2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works we've hidden something with this default syntax: the coefficients are computed for an empty context which might not be what we want to do."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify a fixed context, other than the empty string we have to use a `SingleContextDistribution`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.distributions.single_context_distribution import SingleContextDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incipit = \"It was a cold and stormy night\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([amazing_scorer], [1/2],\n",
    "        n_samples=2**10,\n",
    "        context_distribution=SingleContextDistribution(incipit))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to compute those coefficients for variable contexts, we can use a ContextDistribution to specify a text file listing them, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.distributions.context_distribution import ContextDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([amazing_scorer], [1/2],\n",
    "        n_samples=2**9,\n",
    "        context_distribution=ContextDistribution(\"data/incipits.txt\"), context_sampling_size=2**3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we peek inside the EBM we can check the coefficient that's been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.scorers[1].coefficients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this knowledge, we could define our EBM directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.scorers.exponential_scorer import ExponentialScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base * ExponentialScorer([is_amazing], [6])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Scorers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Scorers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go beyond the very simple example of looking for \"amazing\" in our samples.  \n",
    "A first thing we can do is have multiple such features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_rainy = lambda s, c: bool(re.search(r\"\\brain\\b\", s.text))\n",
    "rainy_scorer = BooleanScorer(is_rainy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this second scorer, we can constrain our base model, wishing for 50% of the samples with the word \"amazing\" and 33% with the \"rain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([amazing_scorer, rainy_scorer], [1/2, 1/3],\n",
    "        n_samples=2**10,\n",
    "        context_distribution=SingleContextDistribution(incipit))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function-based Scorer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Readability</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try something a bit ambitious than just looking for the presence of \"amazing\", be it the string or the word. What about readability? If we can score our samples using for example a FOG index we might use that as a feature, a preference expressed in an EBM."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are useful packages available to compute readability index but we can try to define our own functions: the [FOG](https://en.wikipedia.org/wiki/Gunning_fog_index) is a classic measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text):\n",
    "    return re.findall(r'\\w+', text)\n",
    "\n",
    "def count_sentences(text):\n",
    "    if \"\" == text:\n",
    "        return 0\n",
    "    marks = set(\".!?\")\n",
    "    rw_lngth = len([l for l in text if l in marks])\n",
    "    return rw_lngth if 0 < rw_lngth else 1\n",
    "\n",
    "def count_syllables(word):\n",
    "    vowels = set(\"aeiou\")\n",
    "    return len([l for l in word if l.lower() in vowels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fog(text):\n",
    "    l, h = 6, 17\n",
    "    if \"\" == text:\n",
    "        return l\n",
    "    wrds = extract_words(text)\n",
    "    n_wrds = len(wrds)\n",
    "    n_cmplx_wrds = len([w for w in wrds if 3 < count_syllables(w)])\n",
    "    n_sntncs = count_sentences(text)\n",
    "    rw_scr = round(0.4 * (n_wrds / n_sntncs + 100 * n_cmplx_wrds / n_wrds))\n",
    "    return min(h, max(l, rw_scr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_wars = \"\"\"It is a period of civil war. Rebel spaceships,\n",
    "striking from a hidden base, have won their first victory against\n",
    "the evil Galactic Empire. During the battle, Rebel spies managed\n",
    "to steal secret plans to the Empireâ€™s ultimate weapon, the DEATH STAR,\n",
    "an armoured space station with enough power to destroy an entire planet.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fog(star_wars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to use this `fog()` is to pass is to a BooleanScorer, in a scoring predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base * BooleanScorer(lambda s, c: True if 13 > fog(s.text) else False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, to make things a bit clearer, if a bit more verbose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy(s, _):\n",
    "    \"\"\"a FOG index lower than 13 means that the text\n",
    "    should be readable without college education\"\"\"\n",
    "    return True if 13 > fog(s.text) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base * BooleanScorer(easy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what that gives for a few samples from a default GPT-2, using our infamous [incipit](https://en.wikipedia.org/wiki/It_was_a_dark_and_stormy_night)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal = LMDistribution()\n",
    "samples, _ = proposal.sample(context=incipit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.log_score(samples, context=incipit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined pointwise constraints with our products so we expect all generated samples to be easily readable.  \n",
    "Obviously we could define a distributional constraints, asking for only half of our samples to be easy for example. What would make even more sense here is state that we want our sentences to have _on average_ a FOG index corresponding to the end of high school."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we're going to use a more generic `PositiveScorer` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disco.scorers.positive_scorer import PositiveScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([PositiveScorer(lambda s, c: fog(s.text))], [12],\n",
    "    context_distribution=SingleContextDistribution(incipit))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, again, using a file of incipits for a variable contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([PositiveScorer(lambda s, c: fog(s.text))], [12],\n",
    "        n_samples=2**9,\n",
    "        context_distribution=ContextDistribution(\"data/incipits.txt\"), context_sampling_size=2**3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another way to define our feature is to subclass a `PositiveScorer` in our own `FogScorer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FogScorer(PositiveScorer):\n",
    "    \"\"\"\n",
    "    FOG scoring class\n",
    "    \"\"\"\n",
    "\n",
    "    def _extract_words(self, text):\n",
    "        return re.findall(r'\\w+', text)\n",
    "\n",
    "    def _count_sentences(self, text):\n",
    "        if \"\" == text:\n",
    "            return 0\n",
    "        marks = set(\".!?\")\n",
    "        rw_lngth = len([l for l in text if l in marks])\n",
    "        return rw_lngth if 0 < rw_lngth else 1\n",
    "\n",
    "    def _count_syllables(self, word):\n",
    "        vowels = set(\"aeiou\")\n",
    "        return len([l for l in word if l.lower() in vowels])\n",
    "\n",
    "    def fog(self, sample, _):\n",
    "        text = sample.text\n",
    "        l, h = 6, 17\n",
    "        if \"\" == text:\n",
    "            return l\n",
    "        wrds = self._extract_words(text)\n",
    "        n_wrds = len(wrds)\n",
    "        n_cmplx_wrds = len([w for w in wrds if 3 < self._count_syllables(w)])\n",
    "        n_sntncs = self._count_sentences(text)\n",
    "        rw_scr = round(0.4 * (n_wrds / n_sntncs + 100 * n_cmplx_wrds / n_wrds))\n",
    "        return min(h, max(l, rw_scr))\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scoring_function = self._broadcast(self.fog)\n",
    "\n",
    "    def log_score(self, samples, context):\n",
    "        return torch.log(self.score(samples, context))\n",
    "\n",
    "    def score(self, samples, context):\n",
    "        return torch.tensor(self.scoring_function(samples, context))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be used very similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = base.constrain([FogScorer()], [12],\n",
    "        n_samples=2**9,\n",
    "        context_distribution=ContextDistribution(\"data/incipits.txt\"), context_sampling_size=2**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:38:29) [Clang 13.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "babb4baf4e80bd80b9852210fc5469c0783907e52a560ed7247caef52808358d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
